{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In Code\n",
        "We know how fine-tuning with NSP and MLM works, but how exactly do we apply that in code?\n",
        "\n",
        "Well, we can start by importing transformers, PyTorch, and our training data — Meditations (find a copy of the training data here)."
      ],
      "metadata": {
        "id": "1V6lpDfHo61w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate -U transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htHfhp9UoxJV",
        "outputId": "444405bd-0fe0-4be7-ab55-54e278d3ddc6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H4QIBHQKoe1",
        "outputId": "b70b27bf-03c7-4045-cebd-7390e9f6ad95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForPreTraining\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "\n",
        "with open('/content/drive/MyDrive/data.txt', 'r') as fp:\n",
        "    text = fp.read().split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a list of paragraphs in text — some, but not all, contain multiple sentences. Which we need when building our NSP training data.\n",
        "\n",
        "# Preparing For NSP\n",
        "\n",
        "To prepare our data for NSP, we need to create a mix of non-random sentences (where the two sentences were originally together) — and random sentences.\n",
        "\n",
        "\n",
        "For this, we’ll create a bag of sentences extracted from text which we can then randomly select a sentence from when creating a random NotNextSentence pair."
      ],
      "metadata": {
        "id": "yAqszFoTraxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag = [sentence for para in text for sentence in para.split('.') if sentence != '']\n",
        "bag_size = len(bag)"
      ],
      "metadata": {
        "id": "Qcn4GWJkqQRr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:14]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1KZEw7mqQW9",
        "outputId": "da800c65-25b5-4d35-80a5-2ea005d4418d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['From my grandfather Verus I learned good morals and the government of my temper.',\n",
              " 'From the reputation and remembrance of my father, modesty and a manly character.',\n",
              " 'From my mother, piety and beneficence, and abstinence, not only from evil deeds, but even from evil thoughts; and further, simplicity in my way of living, far removed from the habits of the rich.',\n",
              " 'From my great-grandfather, not to have frequented public schools, and to have had good teachers at home, and to know that on such things a man should spend liberally.',\n",
              " \"From my governor, to be neither of the green nor of the blue party at the games in the Circus, nor a partizan either of the Parmularius or the Scutarius at the gladiators' fights; from him too I learned endurance of labour, and to want little, and to work with my own hands, and not to meddle with other people's affairs, and not to be ready to listen to slander.\",\n",
              " 'From Diognetus, not to busy myself about trifling things, and not to give credit to what was said by miracle-workers and jugglers about incantations and the driving away of daemons and such things; and not to breed quails for fighting, nor to give myself up passionately to such things; and to endure freedom of speech; and to have become intimate with philosophy; and to have been a hearer, first of Bacchius, then of Tandasis and Marcianus; and to have written dialogues in my youth; and to have desired a plank bed and skin, and whatever else of the kind belongs to the Grecian discipline.',\n",
              " 'From Rusticus I received the impression that my character required improvement and discipline; and from him I learned not to be led astray to sophistic emulation, nor to writing on speculative matters, nor to delivering little hortatory orations, nor to showing myself off as a man who practises much discipline, or does benevolent acts in order to make a display; and to abstain from rhetoric, and poetry, and fine writing; and not to walk about in the house in my outdoor dress, nor to do other things of the kind; and to write my letters with simplicity, like the letter which Rusticus wrote from Sinuessa to my mother; and with respect to those who have offended me by words, or done me wrong, to be easily disposed to be pacified and reconciled, as soon as they have shown a readiness to be reconciled; and to read carefully, and not to be satisfied with a superficial understanding of a book; nor hastily to give my assent to those who talk overmuch; and I am indebted to him for being acquainted with the discourses of Epictetus, which he communicated to me out of his own collection.',\n",
              " 'From Apollonius I learned freedom of will and undeviating steadiness of purpose; and to look to nothing else, not even for a moment, except to reason; and to be always the same, in sharp pains, on the occasion of the loss of a child, and in long illness; and to see clearly in a living example that the same man can be both most resolute and yielding, and not peevish in giving his instruction; and to have had before my eyes a man who clearly considered his experience and his skill in expounding philosophical principles as the smallest of his merits; and from him I learned how to receive from friends what are esteemed favours, without being either humbled by them or letting them pass unnoticed.',\n",
              " 'From Sextus, a benevolent disposition, and the example of a family governed in a fatherly manner, and the idea of living conformably to nature; and gravity without affectation, and to look carefully after the interests of friends, and to tolerate ignorant persons, and those who form opinions without consideration: he had the power of readily accommodating himself to all, so that intercourse with him was more agreeable than any flattery; and at the same time he was most highly venerated by those who associated with him: and he had the faculty both of discovering and ordering, in an intelligent and methodical way, the principles necessary for life; and he never showed anger or any other passion, but was entirely free from passion, and also most affectionate; and he could express approbation without noisy display, and he possessed much knowledge without ostentation.',\n",
              " 'From Alexander the grammarian, to refrain from fault-finding, and not in a reproachful way to chide those who uttered any barbarous or solecistic or strange-sounding expression; but dexterously to introduce the very expression which ought to have been used, and in the way of answer or giving confirmation, or joining in an inquiry about the thing itself, not about the word, or by some other fit suggestion.',\n",
              " 'From Fronto I learned to observe what envy, and duplicity, and hypocrisy are in a tyrant, and that generally those among us who are called Patricians are rather deficient in paternal affection.',\n",
              " 'From Alexander the Platonic, not frequently nor without necessity to say to any one, or to write in a letter, that I have no leisure; nor continually to excuse the neglect of duties required by our relation to those with whom we live, by alleging urgent occupations.',\n",
              " 'From Catulus, not to be indifferent when a friend finds fault, even if he should find fault without reason, but to try to restore him to his usual disposition; and to be ready to speak well of teachers, as it is reported of Domitius and Athenodotus; and to love my children truly.',\n",
              " 'From my brother Severus, to love my kin, and to love truth, and to love justice; and through him I learned to know Thrasea, Helvidius, Cato, Dion, Brutus; and from him I received the idea of a polity in which there is the same law for all, a polity administered with regard to equal rights and equal freedom of speech, and the idea of a kingly government which respects most of all the freedom of the governed; I learned from him also consistency and undeviating steadiness in my regard for philosophy; and a disposition to do good, and to give to others readily, and to cherish good hopes, and to believe that I am loved by my friends; and in him I observed no concealment of his opinions with respect to those whom he condemned, and that his friends had no need to conjecture what he wished or did not wish, but it was quite plain.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[9:14]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUsw0CmFqQbB",
        "outputId": "a669c4be-6b28-4661-8cda-3c557cbc87bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['From Alexander the grammarian, to refrain from fault-finding, and not in a reproachful way to chide those who uttered any barbarous or solecistic or strange-sounding expression; but dexterously to introduce the very expression which ought to have been used, and in the way of answer or giving confirmation, or joining in an inquiry about the thing itself, not about the word, or by some other fit suggestion.',\n",
              " 'From Fronto I learned to observe what envy, and duplicity, and hypocrisy are in a tyrant, and that generally those among us who are called Patricians are rather deficient in paternal affection.',\n",
              " 'From Alexander the Platonic, not frequently nor without necessity to say to any one, or to write in a letter, that I have no leisure; nor continually to excuse the neglect of duties required by our relation to those with whom we live, by alleging urgent occupations.',\n",
              " 'From Catulus, not to be indifferent when a friend finds fault, even if he should find fault without reason, but to try to restore him to his usual disposition; and to be ready to speak well of teachers, as it is reported of Domitius and Athenodotus; and to love my children truly.',\n",
              " 'From my brother Severus, to love my kin, and to love truth, and to love justice; and through him I learned to know Thrasea, Helvidius, Cato, Dion, Brutus; and from him I received the idea of a polity in which there is the same law for all, a polity administered with regard to equal rights and equal freedom of speech, and the idea of a kingly government which respects most of all the freedom of the governed; I learned from him also consistency and undeviating steadiness in my regard for philosophy; and a disposition to do good, and to give to others readily, and to cherish good hopes, and to believe that I am loved by my friends; and in him I observed no concealment of his opinions with respect to those whom he condemned, and that his friends had no need to conjecture what he wished or did not wish, but it was quite plain.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating our bag we can go ahead and create our 50/50 random/non-random NSP training data. For this, we will create a list of sentence As, sentence Bs, and their respective IsNextSentence or NotNextSentence labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "5O1CeDeFsTCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "sentence_a = []\n",
        "sentence_b = []\n",
        "label = []\n",
        "\n",
        "for paragraph in text:\n",
        "    sentences = [\n",
        "        sentence for sentence in paragraph.split('.') if sentence != ''\n",
        "    ]\n",
        "    num_sentences = len(sentences)\n",
        "    if num_sentences > 1:\n",
        "      start = random.randint(0, num_sentences-2)\n",
        "      # 50/50 whether is IsNextSentence or NotNextSentence\n",
        "      if random.random() >= 0.5:\n",
        "        # this is IsNextSentence\n",
        "        sentence_a.append(sentences[start])\n",
        "        sentence_b.append(sentences[start+1])\n",
        "        label.append(0)\n",
        "      else:\n",
        "        index = random.randint(0, bag_size-1)\n",
        "        # this is NotNextSentence\n",
        "        sentence_a.append(sentences[start])\n",
        "        sentence_b.append(bag[index])\n",
        "        label.append(1)\n"
      ],
      "metadata": {
        "id": "-aYsbj86pP0f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what we have:\n",
        "\n"
      ],
      "metadata": {
        "id": "l5x3UdHbvUM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  print(sentence_a[i])\n",
        "  print(sentence_b[i])\n",
        "  print(label[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f58dbnSmvTpf",
        "outputId": "9ac01501-0448-46f6-ca83-87b486a29ee7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I observed that everybody believed that he thought as he spoke, and that in all that he did he never had any bad intention; and he never showed amazement and surprise, and was never in a hurry, and never put off doing a thing, nor was perplexed nor dejected, nor did he ever laugh to disguise his vexation, nor, on the other hand, was he ever passionate or suspicious\n",
            " Again, remove to the times of Trajan\n",
            "1\n",
            " He was most ready to give way without envy to those who possessed any particular faculty, such as that of eloquence or knowledge of the law or of morals, or of anything else; and he gave them his help, that each might enjoy reputation according to his deserts; and he always acted conformably to the institutions of his country, without showing any affectation of doing so\n",
            " Strive to continue to be such as philosophy wished to make thee\n",
            "1\n",
            " Further, I owe it to the gods that I was not hurried into any offence against any of them, though I had a disposition which, if opportunity had offered, might have led me to do something of this kind; but, through their favour, there never was such a concurrence of circumstances as put me to the trial\n",
            "- Nothing will stand in the way of thy acting justly and soberly and considerately\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see in the console output that we have label 1 representing random sentences (NotNextSentence) and 0 representing non-random sentences (IsNextSentence).\n",
        "\n",
        "\n",
        "\n",
        "Tokenization\n",
        "We can now tokenize our data. As is typical with BERT models, we truncate/pad our sequences to a length of 512 tokens.\n",
        "\n"
      ],
      "metadata": {
        "id": "t-Gw1FA6v3m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(sentence_a, sentence_b, return_tensors= 'pt', max_length = 512, truncation = True, padding = 'max_length')"
      ],
      "metadata": {
        "id": "S9zzpMn9sLNH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6zprjWsLQx",
        "outputId": "d32b0c40-e437-4675-c821-2857f4024419"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXiuJ0XDsLUr",
        "outputId": "a1f70d1d-6f48-4b84-dc62-bc0c9cd56d53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1045,  5159,  ...,     0,     0,     0],\n",
              "        [  101,  2002,  2001,  ...,     0,     0,     0],\n",
              "        [  101,  2582,  1010,  ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  101,  3459,  2185,  ...,     0,     0,     0],\n",
              "        [  101,  2043, 15223,  ...,     0,     0,     0],\n",
              "        [  101,  7887,  3288,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few things we should take note of here. Because we tokenized two sentences, our tokenizer automatically applied 0 values to sentence A and 1 values to sentence B in the token_type_ids tensor. The trailing zeros are aligned to the padding tokens.\n",
        "\n",
        "\n",
        "Secondly, in the input_ids tensor, the tokenizer automatically placed a SEP token (102) between these two sentences — marking the boundary between them both.\n",
        "\n",
        "\n",
        "BERT needs to see both of these when performing NSP.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S6MgvvHqwyVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# NSP Labels\n",
        "Our NSP labels must be placed within a tensor called `next_sentence_label`. We create this easily by taking our label variable, and converting it into a `torch.LongTensor` — which must also be transposed using .`T`:"
      ],
      "metadata": {
        "id": "-kU50QwVw5ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['next_sentence_label'] = torch.LongTensor([label]).T"
      ],
      "metadata": {
        "id": "jvPGsBzOxqkT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.next_sentence_label[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1T4P3WisLc1",
        "outputId": "f2c23a8e-f98b-4443-fd5a-d531ac3005d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [0],\n",
              "        [0],\n",
              "        [1],\n",
              "        [0],\n",
              "        [1],\n",
              "        [0],\n",
              "        [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masking For MLM\n",
        "For MLM we need to clone our current input_ids tensor to create a MLM labels tensor — then we move onto masking ~15% of tokens in the input_ids tensor."
      ],
      "metadata": {
        "id": "AtGrRV-ByQOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['labels'] = inputs.input_ids.detach().clone()"
      ],
      "metadata": {
        "id": "QBSDGGSypP74"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju91tAQ3pP_b",
        "outputId": "a72ede1a-11bd-45e3-be8f-b5acf90ea2b7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'next_sentence_label', 'labels'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we that clone for our labels, we mask tokens in input_ids.\n"
      ],
      "metadata": {
        "id": "FbKLFbxHy8z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create random array of floats with equal dimensions to input_ids tensor\n",
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "# create mask array\n",
        "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
        "           (inputs.input_ids != 102) * (inputs.input_ids != 0)\n"
      ],
      "metadata": {
        "id": "7G7UcM_3pQDY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now take the indices of each True value within each vector.\n",
        "\n"
      ],
      "metadata": {
        "id": "MtmoFqrt0qZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selection = []\n",
        "\n",
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    selection.append(\n",
        "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "    )\n"
      ],
      "metadata": {
        "id": "af9HFFQm0vJk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selection[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQpl2WVR0vNO",
        "outputId": "8dccae19-1479-49d4-9b83-109e306a0f61"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[13, 19, 49, 53, 61, 73, 75, 86], [2, 32, 37, 39, 44, 63, 65, 67, 71, 76, 85]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Then apply these indices to each row in input_ids, assigning each value at these indices a value of 103."
      ],
      "metadata": {
        "id": "451VBIO11sk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    inputs.input_ids[i, selection[i]] = 103"
      ],
      "metadata": {
        "id": "aAJljv5L6wke"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UTCg5xT1rUA",
        "outputId": "55ac7e21-b70f-4686-8bb0-79b10c382796"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'next_sentence_label', 'labels'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.input_ids\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kifIfLf21rzB",
        "outputId": "500957c9-f636-4025-f244-14cb320a81be"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101,  1045,  5159,  ...,     0,     0,     0],\n",
              "        [  101,  2002,   103,  ...,     0,     0,     0],\n",
              "        [  101,  2582,  1010,  ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  101,  3459,  2185,  ...,     0,     0,     0],\n",
              "        [  101,  2043, 15223,  ...,     0,     0,     0],\n",
              "        [  101,  7887,  3288,  ...,     0,     0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that there are a few rules we’ve added here, by adding the additional logic when creating mask_arr — we are ensuring that we don’t mask any special tokens — such as CLS (101), SEP (102), and PAD (0) tokens.\n",
        "\n",
        "\n",
        "# Dataloader\n",
        "All of our input and label tensors are ready — all we need to do now is format them into a PyTorch dataset object so that it can be loaded into a PyTorch Dataloader — which will feed batches of data into our model during training.\n",
        "\n"
      ],
      "metadata": {
        "id": "J7rCPucm89wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OurDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n"
      ],
      "metadata": {
        "id": "RJHDYIKx1r3C"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = OurDataset(inputs)\n"
      ],
      "metadata": {
        "id": "VHa9JpYJ1r7J"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "IRU20p4I1r_c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataloader expects the __len__ method for checking the total number of samples within our dataset, and the __getitem__ method for extracting samples.\n",
        "\n",
        "# Setup For Training\n",
        "The last step before moving onto our training loop is preparing our model training setup.\n",
        "\n",
        "We first check if we have a GPU available, if so we move the model over to it for training. Then we activate training parameters in our model and initialize an Adam optimizer with weighted decay.\n",
        "\n"
      ],
      "metadata": {
        "id": "3fqR5kmD9bot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# and move our model over to the selected device\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE6Tdaqw-Mtf",
        "outputId": "b507059c-b9b2-430f-aeb4-e1acbf808f85"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForPreTraining(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (cls): BertPreTrainingHeads(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "    )\n",
              "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# activate training mode\n",
        "model.train()\n",
        "# initialize optimizer\n",
        "optim = AdamW(model.parameters(), lr=5e-6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WASBzgZm-UeC",
        "outputId": "c6b5eeee-4c02-4b94-fd4d-1ecad789297c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "Finally, we’re onto training our model. We train for two epochs, and use tqdm to create a progress bar for our training loop."
      ],
      "metadata": {
        "id": "eaLdyzdc-u89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # for our progress bar\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # setup loop with TQDM and dataloader\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch in loop:\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optim.zero_grad()\n",
        "        # pull all tensor batches required for training\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        token_type_ids = batch['token_type_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        next_sentence_label = batch['next_sentence_label'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        # process\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                        token_type_ids=token_type_ids,\n",
        "                        next_sentence_label=next_sentence_label,\n",
        "                        labels=labels)\n",
        "        # extract loss\n",
        "        loss = outputs.loss\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "        # print relevant info to progress bar\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw7LjyYU9xEf",
        "outputId": "00401eb1-6bc0-4a23-c1b9-23d5ed2f34df"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/159 [00:00<?, ?it/s]<ipython-input-22-e49ce9701819>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "Epoch 0: 100%|██████████| 159/159 [37:34<00:00, 14.18s/it, loss=6.52]\n",
            "Epoch 1: 100%|██████████| 159/159 [37:58<00:00, 14.33s/it, loss=2.02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within the loop we:\n",
        "\n",
        "- Initialize gradients, so that we are not starting from the gradients calculated in the previous step.\n",
        "- Move all batch tensors to the selected device (GPU or CPU).\n",
        "- Feed everything into the model and extract loss.\n",
        "- Use loss.backward() to calculate the loss for each parameter.\n",
        "- Update parameter weights based on the calculated loss.\n",
        "- Print relevant information to the progress bar (loop).\n",
        "\n",
        "-And that’s it, with that we’ve fine-tuned our model using both MLM and NSP!\n",
        "\n"
      ],
      "metadata": {
        "id": "WBtnFztHRT4h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V23zkR-xRqw_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}